# Vast.ai Template: LLaMA Factory base + Vast.ai CLI
# Use on Vast.ai via SSH (no portal features, but full PyTorch/NCCL support)
ARG LLAMAFACTORY_VERSION=0.9.4
FROM hiyouga/llamafactory:${LLAMAFACTORY_VERSION}

ENV DEBIAN_FRONTEND=noninteractive

# Install Vast.ai CLI and dependencies
RUN apt-get update && apt-get install -y --no-install-recommends curl ca-certificates && \
    pip install --no-cache-dir vastai && \
    apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/*

# Install logging/tracking tools + flash-attention
ARG FLASH_ATTN_VERSION=2.7.4
ARG FLASH_ATTN_WHEEL=https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl
RUN pip install --no-cache-dir wandb mlflow "optimum>=1.24.0" && \
    (pip install --no-cache-dir --no-deps ${FLASH_ATTN_WHEEL} \
    || echo "WARNING: flash-attention ${FLASH_ATTN_VERSION} install failed - continuing without it") && \
    (pip install --no-cache-dir --no-deps "gptqmodel>=2.0.0" logbar \
    || echo "WARNING: gptqmodel install failed - install at runtime with: pip install --no-deps gptqmodel logbar") && \
    rm -rf /root/.cache/pip /tmp/*

WORKDIR /workspace

# Ports: TensorBoard(6006)
EXPOSE 6006

# Persist env vars and auto-login wandb
COPY scripts/vastai-env.sh /etc/profile.d/vastai-env.sh
RUN chmod +x /etc/profile.d/vastai-env.sh

CMD ["bash"]
